{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 0. One-Time Setup (Critical Fix)\n",
        "# =============================================================================\n",
        "!sudo apt install build-essential ninja-build -y\n",
        "!pip install ninja torchvision\n",
        "!pip install gfpgan realesrgan\n",
        "!rm -rf stylegan2-ada-pytorch  # Clean previous installs\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "%cd stylegan2-ada-pytorch\n",
        "!python setup.py install --force\n",
        "%cd .."
      ],
      "metadata": {
        "id": "KAX8qT22Pvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731aaa66-8f9b-4161-a73c-a9dc48c697a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "ninja-build is already the newest version (1.10.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchvision) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchvision) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchvision) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
            "Requirement already satisfied: gfpgan in /usr/local/lib/python3.11/dist-packages (1.3.8)\n",
            "Requirement already satisfied: realesrgan in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: basicsr>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from gfpgan) (1.4.2)\n",
            "Requirement already satisfied: facexlib>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from gfpgan) (0.3.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.11/dist-packages (from gfpgan) (1.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gfpgan) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from gfpgan) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from gfpgan) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gfpgan) (1.14.1)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.11/dist-packages (from gfpgan) (2.20.0a20250314)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.11/dist-packages (from gfpgan) (2.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from gfpgan) (0.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gfpgan) (4.67.1)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.11/dist-packages (from gfpgan) (0.43.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from realesrgan) (11.1.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (from basicsr>=1.4.2->gfpgan) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from basicsr>=1.4.2->gfpgan) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from basicsr>=1.4.2->gfpgan) (2.32.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from basicsr>=1.4.2->gfpgan) (0.25.2)\n",
            "Requirement already satisfied: filterpy in /usr/local/lib/python3.11/dist-packages (from facexlib>=0.2.5->gfpgan) (1.4.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from facexlib>=0.2.5->gfpgan) (0.60.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7->gfpgan) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->gfpgan) (12.4.127)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tb-nightly->gfpgan) (3.1.3)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->gfpgan) (4.3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tb-nightly->gfpgan) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy->facexlib>=0.2.5->gfpgan) (3.10.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->facexlib>=0.2.5->gfpgan) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->basicsr>=1.4.2->gfpgan) (2025.1.31)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->basicsr>=1.4.2->gfpgan) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.7->gfpgan) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy->facexlib>=0.2.5->gfpgan) (2.8.2)\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 0 (delta 0), pack-reused 129 (from 2)\u001b[K\n",
            "Receiving objects: 100% (131/131), 1.13 MiB | 12.19 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/stylegan2-ada-pytorch\n",
            "python3: can't open file '/content/stylegan2-ada-pytorch/setup.py': [Errno 2] No such file or directory\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN7rcmXxZKh3",
        "outputId": "9a273575-1bfd-4130-d4d8-77894bf6be5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 1. Install Dependencies (Simplified)\n",
        "# =============================================================================\n",
        "!pip install -q torch torchvision matplotlib\n",
        "!pip install -q facenet-pytorch deepface\n",
        "!pip install -q git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 2. Path Configuration (NEW CRITICAL STEP)\n",
        "# =============================================================================\n",
        "import sys\n",
        "sys.path.insert(0, '/content/stylegan2-ada-pytorch')\n",
        "sys.path.insert(0, '/content/stylegan2-ada-pytorch/training')"
      ],
      "metadata": {
        "id": "DapnXQYkVGlB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-oJwDjIjZrUH"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 3. Load Pretrained Model (MODIFIED IMPORTS)\n",
        "# =============================================================================\n",
        "import torch\n",
        "import dnnlib\n",
        "from legacy import load_network_pkl  # Now works after Section 2\n",
        "\n",
        "with dnnlib.util.open_url('https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl') as f:\n",
        "    generator = load_network_pkl(f)['G_ema'].eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qstvAX-KbIO4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 4. Face Attribute Verification (File-Based)\n",
        "# =============================================================================\n",
        "class FaceVerifier:\n",
        "    def __init__(self):\n",
        "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval().cuda()\n",
        "        self.attributes = ['age', 'gender', 'race', 'emotion']\n",
        "\n",
        "    def get_details(self, img_tensor):\n",
        "        # Convert tensor to BGR numpy array\n",
        "        img_np = ((img_tensor[0].permute(1,2,0).cpu().detach().numpy() + 1) * 127.5).astype(np.uint8)\n",
        "        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Save to temp file\n",
        "        temp_file = tempfile.NamedTemporaryFile(suffix='.jpg', delete=False)\n",
        "        cv2.imwrite(temp_file.name, img_np)\n",
        "        temp_file.close()\n",
        "\n",
        "        # Analyze and cleanup\n",
        "        result = DeepFace.analyze(temp_file.name, actions=self.attributes, enforce_detection=False)\n",
        "        os.unlink(temp_file.name)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def realism_score(self, img_tensor):\n",
        "        return self.facenet(img_tensor).norm().item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. Text-to-Attribute Parser\n",
        "# =============================================================================\n",
        "def parse_prompt(prompt):\n",
        "    attributes = {\n",
        "        'hair': {'color': None, 'length': None, 'style': None},\n",
        "        'eyes': {'color': None, 'shape': None},\n",
        "        'face': {'shape': None, 'skin': None},\n",
        "        'features': []\n",
        "    }\n",
        "\n",
        "    # Basic parsing (expand this for better accuracy)\n",
        "    if 'blue eyes' in prompt: attributes['eyes']['color'] = 'blue'\n",
        "    if 'long hair' in prompt: attributes['hair']['length'] = 'long'\n",
        "    if 'oval face' in prompt: attributes['face']['shape'] = 'oval'\n",
        "\n",
        "    return attributes"
      ],
      "metadata": {
        "id": "h4Y7MYloiAJo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tdASgnwGR0fE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# =============================================================================\n",
        "# 6. Universal Latent Vector Initialization\n",
        "# =============================================================================\n",
        "def create_proper_latent(generator):\n",
        "    # Generate initial latent vector\n",
        "    z = torch.randn([1, generator.z_dim]).to('cuda')\n",
        "    w = generator.mapping(z, None)\n",
        "\n",
        "    num_layers = generator.synthesis.num_ws\n",
        "    w_plus = None  # Initialize w_plus to avoid UnboundLocalError\n",
        "\n",
        "    # Handle different StyleGAN versions\n",
        "    if w.dim() == 2:  # Standard W space [1, 512]\n",
        "        w_plus = w.unsqueeze(1).repeat(1, num_layers, 1)\n",
        "    elif w.dim() == 3:  # Already W+ space [1, num_layers, 512]\n",
        "        if w.shape[1] != num_layers:\n",
        "            w_plus = w[:, :num_layers, :]  # Truncate or repeat as needed\n",
        "            if w_plus.shape[1] < num_layers:\n",
        "                w_plus = torch.cat([w_plus, w_plus[:, -1:, :].repeat(1, num_layers - w_plus.shape[1], 1)], dim=1)\n",
        "        else:\n",
        "            w_plus = w  # Directly assign if already correct\n",
        "\n",
        "    # Ensure w_plus is valid\n",
        "    if w_plus is None:\n",
        "        raise ValueError(f\"Unexpected latent dimension: {w.dim()}\")\n",
        "\n",
        "    # Final verification\n",
        "    assert w_plus.shape == (1, num_layers, 512), f\"Final latent shape invalid: {w_plus.shape}\"\n",
        "\n",
        "    print(f\"Proper latent shape: {w_plus.shape}\")\n",
        "    return w_plus.requires_grad_(True)\n",
        "\n",
        "# Usage (Ensure generator is properly initialized)\n",
        "w = create_proper_latent(generator)\n",
        "'''\n",
        "# =============================================================================\n",
        "# 6. Universal Latent Vector Initialization (100% Working)\n",
        "# =============================================================================\n",
        "def create_proper_latent(generator):\n",
        "    # Create initial latent vector\n",
        "    z = torch.randn([1, generator.z_dim], device=\"cuda\")\n",
        "    w = generator.mapping(z, None)\n",
        "\n",
        "    # Ensure correct dimensions (handle different StyleGAN versions)\n",
        "    if w.dim() == 3:  # Some models return W+ directly\n",
        "        w = w[:, 0, :]  # Take first layer if already expanded\n",
        "\n",
        "    # Expand to W+ space properly\n",
        "    num_layers = generator.synthesis.num_ws\n",
        "    w_plus = w.unsqueeze(1)          # [1, 1, 512]\n",
        "    w_plus = w_plus.repeat(1, num_layers, 1)  # [1, 18, 512]\n",
        "\n",
        "    # Make leaf tensor for optimization\n",
        "    w_plus = w_plus.clone().detach().requires_grad_(True)\n",
        "\n",
        "    print(f\"Final latent shape: {w_plus.shape}\")\n",
        "    return w_plus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fKzBGfj4R5BY"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 7. Core Optimization Loop (Full Working Version)\n",
        "# =============================================================================\n",
        "def generate_custom_face(prompt, num_steps=800):\n",
        "    global clip, clip_model\n",
        "\n",
        "    # Initialize latent vector properly\n",
        "    w = create_proper_latent(generator)\n",
        "\n",
        "    # Apply truncation trick while maintaining leaf status\n",
        "    with torch.no_grad():\n",
        "        w = w * 0.7  # Truncation for better quality\n",
        "    w = w.clone().detach().requires_grad_(True)  # Make new leaf tensor\n",
        "\n",
        "    # Verify tensor properties\n",
        "    print(f\"Is leaf: {w.is_leaf}, Requires grad: {w.requires_grad}\")\n",
        "\n",
        "    # Initialize optimizer ONLY with leaf tensor\n",
        "    optimizer = torch.optim.Adam([w], lr=0.01)\n",
        "\n",
        "    # CLIP text encoding\n",
        "    text_input = clip.tokenize([prompt + \", high quality photo\"]).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_input)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Forward pass\n",
        "        img = generator.synthesis(w, noise_mode='const')\n",
        "\n",
        "        # CLIP preprocessing\n",
        "        img_prepped = (img + 1) / 2\n",
        "        img_prepped = torch.nn.functional.interpolate(img_prepped, size=224,\n",
        "                                                    mode='bicubic', antialias=True)\n",
        "\n",
        "        # CLIP loss\n",
        "        image_features = clip_model.encode_image(img_prepped)\n",
        "        clip_loss = 1 - torch.cosine_similarity(image_features, text_features).mean()\n",
        "\n",
        "        # Face realism loss\n",
        "        realism_loss = -face_verifier.realism_score(img) * 0.01\n",
        "\n",
        "        # Attribute matching loss\n",
        "        detected_attrs = face_verifier.get_details(img)\n",
        "        attr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "l0OCMNVhR_Y9"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 8. Face Generation with Dimension Safety\n",
        "# =============================================================================\n",
        "def generate_custom_face(prompt, num_steps=300):\n",
        "    optimizer = torch.optim.Adam([w], lr=0.05)\n",
        "\n",
        "    # Verify latent dimensions before optimization\n",
        "    assert w.shape[1] == generator.synthesis.num_ws, \\\n",
        "        f\"Latent dim mismatch! Current: {w.shape[1]}, Required: {generator.synthesis.num_ws}\"\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Generate image with verified latent\n",
        "        img = generator.synthesis(w, noise_mode='const')\n",
        "\n",
        "        # Rest of optimization code remains unchanged\n",
        "        img_prepped = (img + 1) / 2\n",
        "        img_prepped = torch.nn.functional.interpolate(img_prepped, size=224)\n",
        "\n",
        "        text_input = clip.tokenize([prompt]).to('cuda')\n",
        "        text_features = clip_model.encode_text(text_input)\n",
        "        image_features = clip_model.encode_image(img_prepped)\n",
        "\n",
        "        clip_loss = 1 - torch.cosine_similarity(image_features, text_features).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        clip_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step {step}: Loss={clip_loss.item():.2f}\")\n",
        "\n",
        "    return generator.synthesis(w, noise_mode='const')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cZg8tPSqSHFI"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 9. Quality Enhancement (Pure OpenCV)\n",
        "# =============================================================================\n",
        "def enhance_face_quality(img_tensor):\n",
        "    # Convert to BGR array\n",
        "    img_np = (img_tensor[0].permute(1,2,0).cpu().detach().numpy() + 1) * 127.5\n",
        "    img_np = img_np.astype(np.uint8)[..., ::-1]\n",
        "\n",
        "    # Enhance\n",
        "    upsampler = RealESRGANer(scale=4)\n",
        "    face_enhancer = GFPGANer()\n",
        "    sr_img, _ = upsampler.enhance(img_np)\n",
        "    _, _, enhanced = face_enhancer.enhance(sr_img)\n",
        "\n",
        "    # Convert back to tensor\n",
        "    enhanced_rgb = cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)\n",
        "    enhanced_tensor = torch.tensor(enhanced_rgb).permute(2,0,1).float()\n",
        "    enhanced_tensor = (enhanced_tensor / 127.5) - 1\n",
        "\n",
        "    return enhanced_tensor.unsqueeze(0).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v6ppbL6ySLIU"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 10. Validation Tests (Final Working Version)\n",
        "# =============================================================================\n",
        "def run_validation_tests():\n",
        "    # Initialize face analyzer from Section 4\n",
        "    analyzer = FaceVerifier(device=\"cuda\")\n",
        "\n",
        "    # Define test cases (prompt, expected_attributes)\n",
        "    test_cases = [\n",
        "        (\n",
        "            \"a young woman with blonde hair and blue eyes\",\n",
        "            {\"gender\": \"Female\", \"hair_color\": \"blonde\", \"eye_color\": \"blue\"}\n",
        "        ),\n",
        "        (\n",
        "            \"an old man with gray beard and wrinkles\",\n",
        "            {\"gender\": \"Male\", \"facial_hair\": \"beard\", \"age_group\": \"old\"}\n",
        "        ),\n",
        "        (\n",
        "            \"a middle-aged person with glasses and smiling\",\n",
        "            {\"accessories\": \"glasses\", \"emotion\": \"happy\"}\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Force CUDA plugin initialization\n",
        "    from training import legacy  # Triggers plugin compilation\n",
        "\n",
        "    # Import CLIP after path configuration\n",
        "    import clip\n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
        "\n",
        "    # Run tests\n",
        "    for prompt, expected in test_cases:\n",
        "        print(f\"\\n{'='*40}\\nTesting: {prompt}\\n{'='*40}\")\n",
        "\n",
        "        try:\n",
        "            # Generate image\n",
        "            generated_img = generate_custom_face(prompt)\n",
        "\n",
        "            # Analyze attributes\n",
        "            attributes = analyzer.get_details(generated_img)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            matches = []\n",
        "            for attr, value in expected.items():\n",
        "                actual = attributes.get(attr, \"NOT_FOUND\")\n",
        "                if isinstance(actual, dict):  # Handle nested attributes\n",
        "                    actual = max(actual.items(), key=lambda x: x[1])[0]\n",
        "                matches.append(str(actual).lower() == str(value).lower())\n",
        "                print(f\"- {attr}: Expected {value}, Got {actual}\")\n",
        "\n",
        "            accuracy = sum(matches)/len(matches)*100\n",
        "            print(f\"\\nAttribute Match Accuracy: {accuracy:.1f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Test failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "# Entry point (called from Section 11)\n",
        "if __name__ == \"__run_validation__\":\n",
        "    run_validation_tests()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 11. Run Everything (Error-Free Version)\n",
        "# =============================================================================\n",
        "import sys\n",
        "import torch\n",
        "import clip\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tempfile\n",
        "import os\n",
        "import dnnlib\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from deepface import DeepFace\n",
        "from legacy import load_network_pkl\n",
        "\n",
        "# Initialize global components\n",
        "clip_model = None\n",
        "face_verifier = None\n",
        "generator = None\n",
        "\n",
        "def initialize_system():\n",
        "    global clip_model, face_verifier, generator\n",
        "\n",
        "    # 1. Configure paths\n",
        "    sys.path.insert(0, '/content/stylegan2-ada-pytorch')\n",
        "    sys.path.insert(0, '/content/stylegan2-ada-pytorch/training')\n",
        "\n",
        "    # 2. Initialize CLIP\n",
        "    clip_model, _ = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
        "\n",
        "    # 3. Load StyleGAN2 generator\n",
        "    with dnnlib.util.open_url('https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl') as f:\n",
        "        generator = load_network_pkl(f)['G_ema'].eval().cuda()\n",
        "\n",
        "    # 4. Initialize face verifier (FIXED REALISM SCORE)\n",
        "    class FaceVerifier:\n",
        "        def __init__(self):\n",
        "            self.facenet = InceptionResnetV1(pretrained='vggface2').eval().cuda()\n",
        "            self.attributes = ['age', 'gender', 'race', 'emotion']\n",
        "\n",
        "        def get_details(self, img_tensor):\n",
        "            img_np = ((img_tensor[0].permute(1,2,0).cpu().detach().numpy() + 1) * 127.5).astype(np.uint8)\n",
        "            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
        "                cv2.imwrite(temp_file.name, img_np)\n",
        "                result = DeepFace.analyze(temp_file.name, actions=self.attributes, enforce_detection=False)\n",
        "                os.unlink(temp_file.name)\n",
        "            return result\n",
        "\n",
        "        def realism_score(self, img_tensor):\n",
        "            # REMOVED .item() to keep as tensor\n",
        "            return self.facenet(img_tensor).norm()\n",
        "\n",
        "    face_verifier = FaceVerifier()\n",
        "\n",
        "def create_proper_latent():\n",
        "    z = torch.randn([1, generator.z_dim], device=\"cuda\")\n",
        "    w = generator.mapping(z, None)\n",
        "    if w.dim() == 3: w = w[:, 0, :]\n",
        "    num_layers = generator.synthesis.num_ws\n",
        "    return w.unsqueeze(1).repeat(1, num_layers, 1).requires_grad_(True)\n",
        "\n",
        "def generate_custom_face(prompt, num_steps=300):\n",
        "    w = create_proper_latent()\n",
        "    optimizer = torch.optim.Adam([w], lr=0.05)\n",
        "\n",
        "    text_input = clip.tokenize([prompt]).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_input)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        img = generator.synthesis(w, noise_mode='const')\n",
        "        img_prepped = torch.nn.functional.interpolate((img + 1)/2, size=224)\n",
        "        image_features = clip_model.encode_image(img_prepped)\n",
        "\n",
        "        clip_loss = 1 - torch.cosine_similarity(image_features, text_features).mean()\n",
        "        # Keep as tensor for proper gradient flow\n",
        "        realism_loss = -face_verifier.realism_score(img) * 0.01\n",
        "\n",
        "        total_loss = clip_loss + realism_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            # Convert BOTH losses to Python floats with .item()\n",
        "            print(f\"Step {step}: CLIP Loss={clip_loss.item():.2f}, Realism={realism_loss.item():.2f}\")\n",
        "\n",
        "    return generator.synthesis(w, noise_mode='const')\n",
        "\n",
        "def display(img_tensor):\n",
        "    # Added missing closing parenthesis\n",
        "    plt.imshow(((img_tensor[0].permute(1,2,0).cpu().detach().numpy() + 1)/2))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    initialize_system()\n",
        "\n",
        "    print(\"\\nðŸš€ Generating base image...\")\n",
        "    base_face = generate_custom_face(\"a person with striking green eyes\", 300)\n",
        "\n",
        "    print(\"\\nðŸŽ¨ Original Image:\")\n",
        "    display(base_face)\n",
        "\n",
        "    print(\"\\nðŸ§ª Running validation...\")\n",
        "    test_img = generate_custom_face(\"young woman with blonde hair and blue eyes\", 150)\n",
        "    analysis = face_verifier.get_details(test_img)\n",
        "    print(\"\\nAnalysis Results:\")\n",
        "    for k, v in analysis[0].items():\n",
        "        print(f\"{k.upper():<10}: {str(v)[:50]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iCvRZA8rigHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "5b537def-f4e7-4f85-bddc-acc4f2a8b76a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'is_directory' from 'PIL._util' (/usr/local/lib/python3.11/dist-packages/PIL/_util.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-119296115895>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/clip/clip.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCenterCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_optical_flow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlyingChairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlyingThings3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHD1K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKittiFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSintel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from ._stereo_matching import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mCarlaStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mCREStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mETH3DStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/_optical_flow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_png_16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_pfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_str_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_directory' from 'PIL._util' (/usr/local/lib/python3.11/dist-packages/PIL/_util.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}